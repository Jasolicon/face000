# 超参数调整实施总结

## ✅ 已实施的调整

### 1. 损失权重调整

**修改文件**: `train_universal.py`

**调整内容**:
- `lambda_id`: `2.0` → `3.0` （增加身份相似度权重）
- `lambda_pose`: `0.3` → `0.2` （降低姿态损失权重）
- `lambda_contrast`: `0.2` → `0.1` （降低对比学习权重，减少波动）
- `lambda_similarity_protection`: `0.5` (硬编码) → `1.0` (可配置参数)

**理由**:
- 增加身份相似度权重以提升 `Val/CosineSimilarity`
- 降低对比学习权重以减少 `Val/Loss_contrast` 的高方差
- 增加相似度保护权重以稳定 `Val/Loss_similarity_protection`

---

### 2. 相似度保护损失改进

**修改文件**: `losses_universal.py`

**改进内容**:
```python
# 改进前
protection_loss = F.relu(original_sim - model_sim).mean()

# 改进后
margin = 0.01  # 允许0.01的下降
protection_loss = F.relu(original_sim - model_sim - margin).mean()
```

**理由**:
- 添加margin允许小的下降，减少不必要的惩罚
- 减少损失波动，使训练更稳定

---

### 3. 相似度保护损失权重可配置

**修改文件**: `losses_universal.py`, `train_universal.py`

**改进内容**:
- 在 `UniversalFaceLoss.__init__` 中添加 `lambda_similarity_protection` 参数
- 在 `train_universal.py` 中添加 `--lambda_similarity_protection` 命令行参数
- 在总损失计算中使用可配置的权重而不是硬编码的 `0.5`

**理由**:
- 使超参数调整更灵活
- 便于实验不同的权重值

---

### 4. 学习率调度策略改进

**修改文件**: `train_universal.py`

**改进内容**:
```python
# 改进前
scheduler = optim.lr_scheduler.StepLR(
    optimizer,
    step_size=20,
    gamma=0.5
)
# 调用: scheduler.step()

# 改进后
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='max',  # 监控余弦相似度（越大越好）
    factor=0.5,  # 学习率减半
    patience=10,  # 10个epoch没有改善就降低学习率
    verbose=True,  # 打印学习率变化
    min_lr=1e-6  # 最小学习率
)
# 调用: scheduler.step(val_cosine_sim)
```

**理由**:
- 根据验证集表现自适应调整学习率
- 当验证集余弦相似度不再提升时自动降低学习率
- 避免学习率下降太快或太慢

---

### 5. 对比学习温度参数调整

**修改文件**: `losses_universal.py`, `train_universal.py`

**改进内容**:
- 默认温度: `0.07` → `0.1`
- 添加 `--temperature` 命令行参数

**理由**:
- 增加温度使对比学习更平滑
- 减少 `Val/Loss_contrast` 的波动

---

## 📊 预期效果

### 改进前（46个epoch）:
- `Val/CosineSimilarity`: 0.31-0.32
- `Val/Loss_similarity_protection`: 波动极大（0-5e-5）
- `Val/Loss_contrast`: 高方差（1.15-1.4）
- `Val/Loss_pose`: 波动（2.4-2.6）

### 改进后（预期）:
- `Val/CosineSimilarity`: **0.35-0.38** （提升）
- `Val/Loss_similarity_protection`: **波动减小，更稳定**
- `Val/Loss_contrast`: **方差降低，更平滑**
- `Val/Loss_pose`: **更稳定**

---

## 🚀 使用方法

### 使用新的默认超参数训练:

```bash
C:/Users/62487/.conda/envs/llm/python.exe d:/Code/face000/train_transformer3D/train_universal.py --data_dir train/datas/file --epochs 150
```

### 自定义超参数训练:

```bash
C:/Users/62487/.conda/envs/llm/python.exe d:/Code/face000/train_transformer3D/train_universal.py --data_dir train/datas/file --epochs 150 --lambda_id 3.0 --lambda_contrast 0.1 --lambda_similarity_protection 1.0 --temperature 0.1
```

### 从checkpoint恢复训练（使用新超参数）:

```bash
C:/Users/62487/.conda/envs/llm/python.exe d:/Code/face000/train_transformer3D/train_universal.py --data_dir train/datas/file --resume train_transformer3D/checkpoints_universal/best_model.pth --lambda_id 3.0 --lambda_contrast 0.1 --lambda_similarity_protection 1.0
```

---

## 📝 关键修改点

### `losses_universal.py`:
1. ✅ 添加 `lambda_similarity_protection` 参数
2. ✅ 改进 `similarity_protection` 损失计算（添加margin）
3. ✅ 调整默认温度: `0.07` → `0.1`
4. ✅ 在总损失中使用可配置的 `lambda_similarity_protection`

### `train_universal.py`:
1. ✅ 更新默认损失权重
2. ✅ 添加 `--lambda_similarity_protection` 参数
3. ✅ 添加 `--temperature` 参数
4. ✅ 修改学习率调度器为 `ReduceLROnPlateau`
5. ✅ 修改 `scheduler.step()` 调用，传入验证集余弦相似度

---

## ⚠️ 注意事项

1. **学习率调度器变化**: 
   - 现在需要在验证后调用 `scheduler.step(val_cosine_sim)`
   - 学习率会根据验证集表现自动调整

2. **损失权重变化**: 
   - 如果从旧checkpoint恢复训练，建议使用新的默认权重
   - 或者通过命令行参数显式指定

3. **温度参数**: 
   - 默认值从 `0.07` 改为 `0.1`
   - 可以通过 `--temperature` 参数调整

4. **相似度保护损失**: 
   - 现在有margin（0.01），允许小的下降
   - 权重从 `0.5` 提升到 `1.0`

---

## 🔍 监控建议

训练时重点关注：
1. **Val/CosineSimilarity**: 应该持续上升，目标 > 0.35
2. **Val/Loss_similarity_protection**: 应该稳定在较低值，波动减小
3. **Val/Loss_contrast**: 应该平滑下降，方差降低
4. **学习率变化**: 当验证集表现不再提升时，学习率会自动降低

---

## 📈 下一步优化方向

如果训练结果仍不理想，可以考虑：

1. **进一步调整损失权重**:
   - 如果 `Val/CosineSimilarity` 仍然较低，可以继续增加 `lambda_id`
   - 如果 `Val/Loss_contrast` 仍然波动大，可以进一步降低 `lambda_contrast`

2. **网络架构调整**:
   - 增加 `transformer_depth`（如果显存允许）
   - 增加 `transformer_mlp_dim`（如果显存允许）
   - 调整 `id_dim` 和 `pose_dim`

3. **训练策略调整**:
   - 调整 `batch_size`
   - 使用混合精度训练（`--use_mixed_precision`）
   - 调整 `weight_decay`

