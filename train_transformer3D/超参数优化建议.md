# Transformer超参数优化建议

## 📊 基于训练曲线的分析

根据您提供的训练曲线，观察到以下现象：

1. **损失快速下降后进入平台期**：前10个epoch快速下降，之后缓慢下降并稳定在0.1左右
2. **余弦相似度快速达到0.8后停滞**：前5-10个epoch快速上升，之后几乎不再提升
3. **验证损失略低于训练损失**：这可能是数据集划分问题（已修复）或正则化不足

## 🎯 Transformer优化策略

### 1. **学习率调度（最重要）**

#### 问题分析
- 当前使用`ReduceLROnPlateau`，在平台期可能过早降低学习率
- 缺少Warmup阶段，可能导致训练初期不稳定
- 学习率5e-5可能偏小，无法充分利用Transformer的学习能力

#### 优化方案：Warmup + Cosine Annealing

```python
# 推荐配置
--lr 1e-4                    # 峰值学习率（从5e-5提高到1e-4）
--warmup_epochs 5           # 前5个epoch线性增加学习率
--warmup_lr 1e-6            # 从很小的值开始
--scheduler_type cosine     # Cosine退火（平滑衰减）
--cosine_eta_min 1e-6       # 最小学习率
```

**原理**：
- **Warmup**：前5个epoch从1e-6线性增加到1e-4，避免初期梯度爆炸
- **Cosine Annealing**：之后按余弦函数平滑衰减，有助于精细调优
- **更高峰值LR**：1e-4配合warmup可以更好地探索参数空间

### 2. **损失函数权重优化**

#### 当前状态
- MSE权重：0.3
- 余弦权重：0.7

#### 优化建议
```python
--mse_weight 0.2            # 进一步降低MSE权重
--cosine_weight 0.8         # 进一步提高余弦权重
```

**原因**：
- 相似度卡在0.8，需要更强调特征方向而非数值差异
- 余弦相似度更能反映人脸识别的本质（特征方向）

### 3. **正则化调整**

#### Dropout
```python
--dropout 0.15              # 从0.1增加到0.15（如果出现过拟合）
```

#### 权重衰减
```python
--weight_decay 1e-4         # 从1e-5增加到1e-4（Transformer标准）
```

**调整策略**：
- 如果修复数据集划分后，验证损失显著高于训练损失 → 增加dropout和weight_decay
- 如果两者都高且接近 → 降低dropout，可能模型容量不足

### 4. **早停机制优化**

```python
--early_stopping_patience 15    # 保持15（合理）
--early_stopping_min_delta 1e-4 # 从1e-5增加到1e-4（避免过早停止）
```

### 5. **混合精度训练（性能优化）**

```python
--use_mixed_precision        # 启用FP16，加速训练
```

**收益**：
- 训练速度提升1.5-2倍
- 显存占用减少约50%
- 可以支持更大的batch_size

## 📝 推荐的训练命令

### 基础优化版本（推荐）

```bash
python train_transformer3D/train_3d.py \
    --data_dir train/datas/file \
    --batch_size 32 \
    --epochs 100 \
    --lr 1e-4 \
    --warmup_epochs 5 \
    --warmup_lr 1e-6 \
    --scheduler_type cosine \
    --cosine_eta_min 1e-6 \
    --weight_decay 1e-4 \
    --dropout 0.15 \
    --mse_weight 0.2 \
    --cosine_weight 0.8 \
    --use_mixed_precision \
    --early_stopping_patience 15 \
    --early_stopping_min_delta 1e-4
```

### 激进优化版本（如果基础版本效果不佳）

```bash
python train_transformer3D/train_3d.py \
    --data_dir train/datas/file \
    --batch_size 32 \
    --epochs 150 \
    --lr 2e-4 \
    --warmup_epochs 10 \
    --warmup_lr 1e-6 \
    --scheduler_type cosine \
    --cosine_eta_min 1e-6 \
    --weight_decay 1e-4 \
    --dropout 0.2 \
    --mse_weight 0.1 \
    --cosine_weight 0.9 \
    --use_mixed_precision \
    --early_stopping_patience 20 \
    --early_stopping_min_delta 1e-4
```

### 保守优化版本（如果训练不稳定）

```bash
python train_transformer3D/train_3d.py \
    --data_dir train/datas/file \
    --batch_size 32 \
    --epochs 100 \
    --lr 5e-5 \
    --warmup_epochs 5 \
    --warmup_lr 1e-6 \
    --scheduler_type cosine \
    --cosine_eta_min 1e-7 \
    --weight_decay 1e-5 \
    --dropout 0.1 \
    --mse_weight 0.3 \
    --cosine_weight 0.7 \
    --use_mixed_precision \
    --early_stopping_patience 15 \
    --early_stopping_min_delta 1e-5
```

## 🔍 预期效果

### 使用Warmup + Cosine后的预期曲线

1. **前5个epoch（Warmup阶段）**：
   - 学习率从1e-6线性增加到1e-4
   - 损失和相似度可能变化较慢（学习率较小）
   - 但训练更稳定，不会出现初期震荡

2. **5-30个epoch（Cosine衰减前期）**：
   - 学习率在1e-4附近，模型快速学习
   - 损失和相似度应该快速改善
   - 相似度有望突破0.8，达到0.85-0.90

3. **30-100个epoch（Cosine衰减后期）**：
   - 学习率平滑下降
   - 模型进行精细调优
   - 相似度有望进一步提升到0.90+

## ⚠️ 注意事项

1. **数据集划分修复后**：验证损失可能会高于训练损失，这是正常的
2. **过拟合判断**：如果验证损失显著高于训练损失（>20%），增加dropout和weight_decay
3. **欠拟合判断**：如果两者都高且接近，考虑：
   - 增加模型容量（d_model, num_layers）
   - 降低dropout
   - 增加训练轮数

## 📈 监控指标

训练时关注：
1. **学习率曲线**：应该看到Warmup阶段的线性增长，然后是Cosine衰减
2. **训练/验证损失比**：理想值在0.9-1.1之间
3. **余弦相似度趋势**：应该持续上升，而不是卡在平台期
4. **梯度范数**：如果出现梯度爆炸，降低峰值学习率

## 🎓 Transformer训练最佳实践总结

1. ✅ **Warmup是必须的**：前5-10%的epoch进行学习率预热
2. ✅ **Cosine Annealing优于Plateau**：平滑衰减，更适合Transformer
3. ✅ **学习率要足够大**：1e-4到2e-4是Transformer的甜点区间
4. ✅ **权重衰减1e-4**：Transformer的标准配置
5. ✅ **Dropout 0.15-0.2**：根据过拟合情况调整
6. ✅ **混合精度训练**：几乎总是有益的
7. ✅ **早停机制**：避免过拟合，节省训练时间
