# ViewDiffé£æ ¼å¢å¼ºæ¨¡å—

å‚è€ƒViewDiffçš„å®ç°ï¼Œä¸ºäººè„¸è§’åº¦è½¬æ¢ä»»åŠ¡æä¾›ä»¥ä¸‹æ”¹è¿›ï¼š

## ğŸ¯ æ ¸å¿ƒæ”¹è¿›

### 1. **LoRAçº¿æ€§å±‚å’Œå§¿æ€æ¡ä»¶LoRAæ³¨æ„åŠ›**
- **LoRALinearLayer**: ä½ç§©é€‚åº”çº¿æ€§å±‚ï¼Œå¤§å¹…å‡å°‘å¯è®­ç»ƒå‚æ•°
- **PoseConditionedLoRAAttention**: å°†å§¿æ€ä¿¡æ¯é€šè¿‡LoRAæ³¨å…¥æ³¨æ„åŠ›æœºåˆ¶

### 2. **è½»é‡åŒ–3DæŠ•å½±å±‚**
- **LightweightFaceProjectionLayer**: åŸºäº3Då…³é”®ç‚¹çš„2Dâ†’3Dâ†’2DæŠ•å½±
- ä½¿ç”¨ä½“ç´ ç½‘æ ¼å®ç°è½»é‡çº§3Dç‰¹å¾è¡¨ç¤º

### 3. **è·¨è§†è§’æ³¨æ„åŠ›æœºåˆ¶**
- **CrossViewAttention**: è®©ä¸åŒè§†è§’çš„ç‰¹å¾ç›¸äº’å…³æ³¨
- æ”¯æŒå¤šè§†è§’æ‰¹å¤„ç†å·¥å…·

### 4. **å…ˆéªŒä¿æŠ¤è®­ç»ƒ**
- **PriorPreservationLoss**: é˜²æ­¢å¾®è°ƒæ—¶ä¸¢å¤±åŸå§‹ç”Ÿæˆèƒ½åŠ›
- **PriorPreservationDataset**: å…ˆéªŒæ ·æœ¬ç”Ÿæˆå’Œç¼“å­˜

### 5. **å®Œæ•´é›†æˆæ¨¡å‹**
- **EnhancedTransformerDecoderOnly3D**: é›†æˆæ‰€æœ‰æ”¹è¿›çš„å¢å¼ºç‰ˆæ¨¡å‹
- **EnhancedTransformerWithPrior**: å¸¦å…ˆéªŒä¿æŠ¤è®­ç»ƒçš„åŒ…è£…å™¨

## ğŸ“ æ–‡ä»¶ç»“æ„

```
viewdiff/
â”œâ”€â”€ __init__.py                      # æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ lora_layers.py                   # LoRAçº¿æ€§å±‚
â”œâ”€â”€ pose_lora_attention.py          # å§¿æ€æ¡ä»¶LoRAæ³¨æ„åŠ›
â”œâ”€â”€ face_projection_layer.py         # è½»é‡åŒ–3DæŠ•å½±å±‚
â”œâ”€â”€ multiview_utils.py              # è·¨è§†è§’æ³¨æ„åŠ›å’Œå·¥å…·å‡½æ•°
â”œâ”€â”€ prior_preservation.py           # å…ˆéªŒä¿æŠ¤æŸå¤±å’Œæ•°æ®é›†
â”œâ”€â”€ enhanced_transformer_3d.py      # å®Œæ•´é›†æˆæ¨¡å‹
â”œâ”€â”€ train_enhanced.py                # è®­ç»ƒè„šæœ¬ï¼ˆå¾…åˆ›å»ºï¼‰
â””â”€â”€ README.md                        # æœ¬æ–‡ä»¶
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºç¡€ä½¿ç”¨

```python
from train_transformer3D.viewdiff import EnhancedTransformerDecoderOnly3D

# åˆ›å»ºå¢å¼ºç‰ˆæ¨¡å‹
model = EnhancedTransformerDecoderOnly3D(
    d_model=512,
    nhead=8,
    num_layers=4,
    num_keypoints=5,
    pose_dim=3,
    use_lora_attention=True,      # å¯ç”¨LoRAæ³¨æ„åŠ›
    use_projection_layer=True,    # å¯ç”¨3DæŠ•å½±å±‚
    use_cross_view=False,         # å•è§†è§’ä»»åŠ¡
    rank=4,                       # LoRAç§©
    lora_alpha=1.0
)

# å‰å‘ä¼ æ’­
output = model(
    src=src_features,           # [batch, 512]
    angles=angles,              # [batch, 3]
    keypoints_3d=keypoints_3d,  # [batch, 5, 3]
    pose=pose                   # [batch, 3]
)
```

### å¸¦å…ˆéªŒä¿æŠ¤çš„è®­ç»ƒ

```python
from train_transformer3D.viewdiff import (
    EnhancedTransformerDecoderOnly3D,
    EnhancedTransformerWithPrior
)

# 1. åˆ›å»ºåŸºç¡€æ¨¡å‹ï¼ˆåŸå§‹é¢„è®­ç»ƒæ¨¡å‹ï¼‰
base_model = EnhancedTransformerDecoderOnly3D(
    d_model=512,
    use_lora_attention=False,  # åŸºç¡€æ¨¡å‹ä¸ä½¿ç”¨LoRA
    use_projection_layer=False
)

# 2. åˆ›å»ºå¢å¼ºæ¨¡å‹
enhanced_model = EnhancedTransformerDecoderOnly3D(
    d_model=512,
    use_lora_attention=True,
    use_projection_layer=True
)

# 3. åŒ…è£…å¸¦å…ˆéªŒä¿æŠ¤çš„æ¨¡å‹
model_with_prior = EnhancedTransformerWithPrior(
    model=enhanced_model,
    base_model=base_model,
    lambda_prior=0.1  # å…ˆéªŒä¿æŠ¤æƒé‡
)

# 4. é…ç½®ä¼˜åŒ–å™¨ï¼ˆå¯å•ç‹¬ä¼˜åŒ–LoRAå‚æ•°ï¼‰
all_params = enhanced_model.get_trainable_parameters()
lora_params = enhanced_model.get_lora_parameters()

optimizer = torch.optim.AdamW([
    {'params': all_params, 'lr': 1e-4},
    {'params': lora_params, 'lr': 1e-3}  # LoRAå‚æ•°ç”¨æ›´é«˜å­¦ä¹ ç‡
])

# 5. è®­ç»ƒå¾ªç¯
for batch in dataloader:
    src, angles, keypoints, pose, targets = batch
    
    # å‰å‘ä¼ æ’­
    outputs = enhanced_model(
        src=src,
        angles=angles,
        keypoints_3d=keypoints,
        pose=pose
    )
    
    # è®¡ç®—æŸå¤±ï¼ˆå¸¦å…ˆéªŒä¿æŠ¤ï¼‰
    def mse_loss(pred, target):
        return F.mse_loss(pred, target)
    
    loss, loss_dict = model_with_prior.compute_loss(
        inputs=src,
        conditions={'angles': angles, 'pose': pose, 'keypoints_3d': keypoints},
        targets=targets,
        original_loss_fn=mse_loss
    )
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

### å¤šè§†è§’è®­ç»ƒ

```python
# ä½¿ç”¨è·¨è§†è§’æ³¨æ„åŠ›
model = EnhancedTransformerDecoderOnly3D(
    d_model=512,
    use_cross_view=True,  # å¯ç”¨è·¨è§†è§’æ³¨æ„åŠ›
    n_views=5            # 5ä¸ªè§†è§’
)

# è¾“å…¥å¿…é¡»æ˜¯n_viewsçš„å€æ•°
# ä¾‹å¦‚ï¼šbatch_size=8, n_views=5, åˆ™å®é™…è¾“å…¥40ä¸ªæ ·æœ¬
output = model(
    src=src_features,  # [40, 512] (8*5)
    angles=angles,
    multiview_input=True  # æ ‡è®°ä¸ºå¤šè§†è§’è¾“å…¥
)
```

## ğŸ”§ å‚æ•°è¯´æ˜

### LoRAå‚æ•°
- `rank`: LoRAç§©ï¼Œé€šå¸¸4-16ï¼Œè¶Šå°å‚æ•°é‡è¶Šå°‘ä½†è¡¨è¾¾èƒ½åŠ›å¯èƒ½é™ä½
- `lora_alpha`: LoRAç¼©æ”¾å› å­ï¼Œæ§åˆ¶é€‚é…å¼ºåº¦ï¼Œé€šå¸¸ç­‰äºrank

### 3DæŠ•å½±å±‚å‚æ•°
- `voxel_resolution`: ä½“ç´ ç½‘æ ¼åˆ†è¾¨ç‡ï¼Œé»˜è®¤16ï¼Œè¶Šå¤§ç²¾åº¦è¶Šé«˜ä½†è®¡ç®—æˆæœ¬å¢åŠ 

### å…ˆéªŒä¿æŠ¤å‚æ•°
- `lambda_prior`: å…ˆéªŒä¿æŠ¤æƒé‡ï¼Œé€šå¸¸0.1-0.5ï¼Œè¶Šå¤§è¶Šä¿å®ˆ

## ğŸ“Š æ€§èƒ½ä¼˜åŠ¿

1. **å‚æ•°é‡å‡å°‘**: LoRAæ³¨æ„åŠ›å¤§å¹…å‡å°‘å¯è®­ç»ƒå‚æ•°ï¼ˆçº¦å‡å°‘70-90%ï¼‰
2. **è®­ç»ƒé€Ÿåº¦**: æ›´å°‘çš„å‚æ•°æ„å‘³ç€æ›´å¿«çš„è®­ç»ƒå’Œæ¨ç†
3. **çµæ´»æ€§**: å¯ä»¥å•ç‹¬å¯ç”¨/ç¦ç”¨å„ä¸ªæ¨¡å—
4. **å…ˆéªŒä¿æŠ¤**: é˜²æ­¢å¾®è°ƒæ—¶è¿‡æ‹Ÿåˆï¼Œä¿æŒæ³›åŒ–èƒ½åŠ›

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **LoRA vs åŸå§‹æ³¨æ„åŠ›**: `use_lora_attention` å’Œ `use_pose_attention` å»ºè®®åªå¯ç”¨ä¸€ä¸ª
2. **å¤šè§†è§’è¾“å…¥**: ä½¿ç”¨è·¨è§†è§’æ³¨æ„åŠ›æ—¶ï¼Œæ‰¹æ¬¡å¤§å°å¿…é¡»æ˜¯ `n_views` çš„å€æ•°
3. **å…ˆéªŒä¿æŠ¤**: éœ€è¦æä¾›åŸºç¡€æ¨¡å‹ï¼Œç¡®ä¿åŸºç¡€æ¨¡å‹å·²å†»ç»“
4. **å†…å­˜ä½¿ç”¨**: 3DæŠ•å½±å±‚ä¼šå¢åŠ å†…å­˜ä½¿ç”¨ï¼Œæ ¹æ®GPUå†…å­˜è°ƒæ•´ `voxel_resolution`

## ğŸ”— å‚è€ƒ

- ViewDiff: https://github.com/...
- DreamBooth: å…ˆéªŒä¿æŠ¤ç­–ç•¥
- LoRA: Low-Rank Adaptation of Large Language Models

