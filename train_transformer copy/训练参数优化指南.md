# 训练参数优化指南

## 概述

本文档提供针对不同场景的训练参数优化建议，帮助提升模型效果。

## 一、快速开始（推荐配置）

### 标准Transformer（最佳效果）

```bash
python train_transformer/train.py \
    --model_type transformer \
    --batch_size 128 \
    --lr 2e-4 \
    --weight_decay 1e-4 \
    --warmup_epochs 5 \
    --use_scheduler \
    --scheduler_type cosine \
    --dropout 0.15 \
    --residual_weight 0.6 \
    --final_weight 0.4 \
    --epochs 150
```

### 轻量级Transformer（平衡效果和速度）

```bash
python train_transformer/train.py \
    --model_type lightweight_transformer \
    --batch_size 128 \
    --lr 2e-4 \
    --weight_decay 1e-4 \
    --warmup_epochs 5 \
    --use_scheduler \
    --scheduler_type cosine \
    --dropout 0.15 \
    --residual_weight 0.6 \
    --final_weight 0.4 \
    --epochs 150
```

---

## 二、参数详解与优化建议

### 1. 学习率 (Learning Rate)

#### 默认值：`2e-4`（已优化）

**调整建议**：

| 模型大小 | 推荐学习率 | 说明 |
|---------|-----------|------|
| 标准Transformer (15M) | `2e-4` | 平衡收敛速度和稳定性 |
| 轻量级Transformer (4M) | `2e-4` 或 `3e-4` | 可以稍大，因为模型小 |
| MLP (2M) | `3e-4` 或 `5e-4` | 可以更大 |
| 残差MLP (1.5M) | `5e-4` | 最小模型，可以用更大学习率 |

**学习率预热 (Warmup)**：
- **默认**：`--warmup_epochs 5`
- **作用**：前5个epoch线性增加学习率，避免初期训练不稳定
- **推荐**：5-10 epochs

**学习率调度器**：
- **推荐**：`--use_scheduler --scheduler_type cosine`
- **优势**：平滑降低学习率，有助于精细调优
- **替代方案**：
  - `plateau`：验证损失不降时降低学习率
  - `step`：固定步长降低学习率

---

### 2. 权重衰减 (Weight Decay)

#### 默认值：`1e-4`（已优化）

**调整建议**：

| 数据量 | 推荐权重衰减 | 说明 |
|-------|------------|------|
| 大数据集 (>10K样本) | `1e-4` | 标准正则化 |
| 中等数据集 (5K-10K) | `1e-4` 或 `5e-5` | 适度正则化 |
| 小数据集 (<5K) | `5e-5` 或 `1e-5` | 减少正则化，避免欠拟合 |

**作用**：防止过拟合，提高泛化能力

---

### 3. 批次大小 (Batch Size)

#### 默认值：`128`（24GB显卡）

**调整建议**：

| GPU显存 | 推荐batch_size | 说明 |
|--------|---------------|------|
| 24GB | `128` 或 `256` | 充分利用显存 |
| 12GB | `64` 或 `128` | 平衡速度和显存 |
| 8GB | `32` 或 `64` | 较小批次 |

**梯度累积**：
- 如果显存不足，可以使用梯度累积模拟更大的batch_size：
  ```bash
  --batch_size 64 --gradient_accumulation_steps 2  # 有效batch_size = 128
  ```

---

### 4. Dropout

#### 默认值：`0.15`（已优化）

**调整建议**：

| 模型大小 | 推荐Dropout | 说明 |
|---------|------------|------|
| 大模型 (>10M) | `0.15` 或 `0.2` | 更多正则化 |
| 中等模型 (5M-10M) | `0.15` | 标准值 |
| 小模型 (<5M) | `0.1` 或 `0.15` | 减少正则化 |

**过拟合判断**：
- 如果训练损失 << 验证损失：增加dropout到 `0.2`
- 如果训练损失 ≈ 验证损失：可以降低dropout到 `0.1`

---

### 5. 损失函数权重

#### 默认值：
- `residual_weight`: `0.6`
- `final_weight`: `0.4`
- `cosine_weight`: `0.5`
- `mse_weight`: `0.5`

**调整建议**：

**残差 vs 最终特征权重**：
- **推荐**：`residual_weight=0.6, final_weight=0.4`
- **原理**：残差学习更稳定，最终特征用于精细调优
- **调整**：
  - 如果模型收敛慢：增加 `residual_weight` 到 `0.7`
  - 如果模型精度不够：增加 `final_weight` 到 `0.5`

**余弦 vs MSE权重**：
- **推荐**：`cosine_weight=0.5, mse_weight=0.5`
- **调整**：
  - 如果特征方向重要：增加 `cosine_weight` 到 `0.6-0.7`
  - 如果特征数值重要：增加 `mse_weight` 到 `0.6-0.7`

---

### 6. 模型架构参数

#### 标准Transformer

```bash
--model_type transformer \
--nhead 8 \
--num_layers 4 \
--dim_feedforward 2048
```

**调整建议**：
- **增加层数**：`--num_layers 6` 或 `8`（提升表达能力，但需要更多显存）
- **增加注意力头**：`--nhead 12` 或 `16`（提升注意力机制）
- **增加前馈维度**：`--dim_feedforward 3072`（提升模型容量）

#### 轻量级Transformer

```bash
--model_type lightweight_transformer \
--nhead 4 \
--num_layers 2 \
--dim_feedforward 1024
```

**调整建议**：
- 如果需要更好效果：增加 `--num_layers 3` 或 `4`
- 如果显存充足：增加 `--nhead 6` 或 `8`

---

### 7. 训练轮数 (Epochs)

#### 默认值：`100`

**调整建议**：

| 数据量 | 推荐轮数 | 说明 |
|-------|---------|------|
| 大数据集 (>10K) | `100-150` | 充分训练 |
| 中等数据集 (5K-10K) | `150-200` | 可能需要更多轮次 |
| 小数据集 (<5K) | `200-300` | 需要更多轮次，注意过拟合 |

**早停 (Early Stopping)**：
- 建议监控验证损失，如果连续20-30个epoch不下降，可以提前停止

---

## 三、不同场景的优化策略

### 场景1：追求最佳效果（不限制训练时间）

```bash
python train_transformer/train.py \
    --model_type transformer \
    --nhead 12 \
    --num_layers 6 \
    --dim_feedforward 3072 \
    --batch_size 128 \
    --lr 2e-4 \
    --weight_decay 1e-4 \
    --warmup_epochs 10 \
    --use_scheduler \
    --scheduler_type cosine \
    --dropout 0.2 \
    --residual_weight 0.6 \
    --final_weight 0.4 \
    --epochs 200
```

### 场景2：快速实验（快速迭代）

```bash
python train_transformer/train.py \
    --model_type lightweight_transformer \
    --batch_size 128 \
    --lr 3e-4 \
    --weight_decay 1e-4 \
    --warmup_epochs 3 \
    --use_scheduler \
    --scheduler_type cosine \
    --dropout 0.15 \
    --epochs 50
```

### 场景3：显存受限（8GB显卡）

```bash
python train_transformer/train.py \
    --model_type lightweight_transformer \
    --batch_size 32 \
    --gradient_accumulation_steps 4 \
    --lr 2e-4 \
    --weight_decay 1e-4 \
    --warmup_epochs 5 \
    --use_scheduler \
    --scheduler_type cosine \
    --dropout 0.15 \
    --epochs 150
```

### 场景4：小数据集（防止过拟合）

```bash
python train_transformer/train.py \
    --model_type lightweight_transformer \
    --batch_size 64 \
    --lr 2e-4 \
    --weight_decay 5e-5 \
    --warmup_epochs 5 \
    --use_scheduler \
    --scheduler_type plateau \
    --scheduler_patience 15 \
    --dropout 0.2 \
    --epochs 200
```

### 场景5：大数据集（充分利用数据）

```bash
python train_transformer/train.py \
    --model_type transformer \
    --batch_size 256 \
    --lr 2e-4 \
    --weight_decay 1e-4 \
    --warmup_epochs 10 \
    --use_scheduler \
    --scheduler_type cosine \
    --dropout 0.15 \
    --epochs 150
```

---

## 四、参数调优流程

### 步骤1：基线训练

使用推荐配置训练，观察训练曲线：
- 训练损失是否下降？
- 验证损失是否下降？
- 是否有过拟合？

### 步骤2：根据问题调整

**问题1：训练损失不下降**
- 增加学习率：`--lr 3e-4` 或 `5e-4`
- 减少权重衰减：`--weight_decay 5e-5`
- 检查数据质量

**问题2：验证损失不下降（过拟合）**
- 增加dropout：`--dropout 0.2`
- 增加权重衰减：`--weight_decay 2e-4`
- 减少模型大小：使用 `lightweight_transformer`
- 增加数据增强

**问题3：收敛太慢**
- 增加学习率：`--lr 3e-4`
- 增加warmup轮数：`--warmup_epochs 10`
- 使用更大的batch_size

**问题4：精度不够**
- 使用更大的模型：`--model_type transformer`
- 增加层数：`--num_layers 6`
- 增加 `final_weight`：`--final_weight 0.5`

### 步骤3：精细调优

在基线基础上微调：
- 学习率：±50%
- 权重衰减：±50%
- Dropout：±0.05
- 损失权重：±0.1

---

## 五、监控指标

### 关键指标

1. **训练损失**：应该持续下降
2. **验证损失**：应该下降，且接近训练损失
3. **验证余弦相似度**：应该上升并稳定在0.85+
4. **学习率**：应该按调度器变化

### 异常情况

- **训练损失 << 验证损失**：过拟合，增加正则化
- **训练损失 ≈ 验证损失，但都很高**：欠拟合，增加模型容量或学习率
- **损失震荡**：学习率太大，降低学习率
- **损失不下降**：学习率太小，增加学习率

---

## 六、最佳实践

1. ✅ **使用学习率预热**：前5-10个epoch线性增加学习率
2. ✅ **使用Cosine调度器**：平滑降低学习率
3. ✅ **监控验证指标**：及时发现问题
4. ✅ **保存最佳模型**：基于验证损失保存
5. ✅ **使用混合精度训练**：默认启用FP16，加速训练
6. ✅ **调整损失权重**：根据任务特点调整
7. ✅ **适当正则化**：防止过拟合

---

## 七、参数对比表

| 参数 | 旧默认值 | 新默认值 | 优化原因 |
|------|---------|---------|---------|
| 学习率 | `1e-4` | `2e-4` | 加快收敛速度 |
| 权重衰减 | `1e-5` | `1e-4` | 更好的正则化 |
| Dropout | `0.1` | `0.15` | 防止过拟合 |
| residual_weight | `0.5` | `0.6` | 更稳定的学习 |
| final_weight | `0.5` | `0.4` | 平衡残差和最终特征 |
| warmup_epochs | `0` | `5` | 训练更稳定 |
| use_scheduler | `False` | `True` | 更好的收敛 |

---

## 总结

通过以上优化，训练效果应该会有明显提升。建议：

1. **从推荐配置开始**：使用标准Transformer + 优化后的参数
2. **观察训练曲线**：根据实际情况调整
3. **逐步优化**：不要同时改变太多参数
4. **记录实验**：保存不同配置的结果，便于对比

祝训练顺利！🎉

